{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8dc9f8e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-12-01T10:45:04.735383Z",
     "iopub.status.busy": "2022-12-01T10:45:04.734504Z",
     "iopub.status.idle": "2022-12-01T10:45:06.564862Z",
     "shell.execute_reply": "2022-12-01T10:45:06.563555Z"
    },
    "papermill": {
     "duration": 1.849463,
     "end_time": "2022-12-01T10:45:06.567750",
     "exception": false,
     "start_time": "2022-12-01T10:45:04.718287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.title_extraction'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# nltk.download('wordnet')\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#from nltk.stem import PorterStemmer, WordNetLemmatizer\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtitle_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer  \n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KNeighborsRegressor\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sqrt\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.title_extraction'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "#from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.title_extraction.text import TfidfVectorizer  \n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from math import sqrt\n",
    "\n",
    "import re\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ce5476a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T10:45:06.596043Z",
     "iopub.status.busy": "2022-12-01T10:45:06.594859Z",
     "iopub.status.idle": "2022-12-01T10:45:06.755593Z",
     "shell.execute_reply": "2022-12-01T10:45:06.754176Z"
    },
    "papermill": {
     "duration": 0.17724,
     "end_time": "2022-12-01T10:45:06.758085",
     "exception": false,
     "start_time": "2022-12-01T10:45:06.580845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17692\\1376211583.py:1: DtypeWarning: Columns (0,3,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_network = pd.read_csv('./citation_network.csv')\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c439a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id         ref  year  year_ref    doc_type  n_citation_ref  n_citation  \\\n",
      "0  1091  2005687710     0         0        none               3           1   \n",
      "1  1091  2018037215     0         0        none              12           1   \n",
      "2  1674  1535888970  2003      2003  Conference              63           1   \n",
      "3  1674  1992876689     0         0        none               5           1   \n",
      "4  1674  1993710814     0         0        none              10           1   \n",
      "\n",
      "   n_citation_ref_count  Point  \\\n",
      "0                     3     32   \n",
      "1                    12    122   \n",
      "2                    12   2662   \n",
      "3                     5     52   \n",
      "4                    10    102   \n",
      "\n",
      "                                               title  \n",
      "0  Preliminary Design of a Network Protocol Learn...  \n",
      "1  Preliminary Design of a Network Protocol Learn...  \n",
      "2  A methodology for the physically accurate visu...  \n",
      "3  A methodology for the physically accurate visu...  \n",
      "4  A methodology for the physically accurate visu...  \n"
     ]
    }
   ],
   "source": [
    "# Ensure 'id' columns in both DataFrames have the same data type (convert to string)\n",
    "df_network['id'] = df_network['id'].astype(str)\n",
    "df_ref['id'] = df_ref['id'].astype(str)\n",
    "\n",
    "# Merge the DataFrames on 'id' and 'ref'\n",
    "df = pd.merge(df_ref, df_network[['id', 'title']], on='id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1a96971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ref</th>\n",
       "      <th>year</th>\n",
       "      <th>year_ref</th>\n",
       "      <th>doc_type</th>\n",
       "      <th>n_citation_ref</th>\n",
       "      <th>n_citation</th>\n",
       "      <th>n_citation_ref_count</th>\n",
       "      <th>Point</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1091</td>\n",
       "      <td>2005687710</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>preliminary design of a network protocol learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1091</td>\n",
       "      <td>2018037215</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>122</td>\n",
       "      <td>preliminary design of a network protocol learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1674</td>\n",
       "      <td>1535888970</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>Conference</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2662</td>\n",
       "      <td>a methodology for the physically accurate visu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1674</td>\n",
       "      <td>1992876689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>a methodology for the physically accurate visu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1674</td>\n",
       "      <td>1993710814</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>102</td>\n",
       "      <td>a methodology for the physically accurate visu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id         ref  year  year_ref    doc_type  n_citation_ref  n_citation  \\\n",
       "0  1091  2005687710     0         0        none               3           1   \n",
       "1  1091  2018037215     0         0        none              12           1   \n",
       "2  1674  1535888970  2003      2003  Conference              63           1   \n",
       "3  1674  1992876689     0         0        none               5           1   \n",
       "4  1674  1993710814     0         0        none              10           1   \n",
       "\n",
       "   n_citation_ref_count  Point  \\\n",
       "0                     3     32   \n",
       "1                    12    122   \n",
       "2                    12   2662   \n",
       "3                     5     52   \n",
       "4                    10    102   \n",
       "\n",
       "                                               title  \n",
       "0  preliminary design of a network protocol learn...  \n",
       "1  preliminary design of a network protocol learn...  \n",
       "2  a methodology for the physically accurate visu...  \n",
       "3  a methodology for the physically accurate visu...  \n",
       "4  a methodology for the physically accurate visu...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff6426eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ee04d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['title'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f287fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                                              title\n",
      "0   1091  preliminary design of a network protocol learn...\n",
      "2   1674  a methodology for the physically accurate visu...\n",
      "17  1688  comparison of garch neural network and support...\n",
      "20  6522  improved secret image sharing method by encodi...\n",
      "24  8373  formal agent oriented ubiquitous computing a c...\n"
     ]
    }
   ],
   "source": [
    "df_unique = df[['id', 'title']].drop_duplicates(subset='id')\n",
    "\n",
    "# Check the first few rows to confirm the results\n",
    "print(df_unique.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bf25654",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 74.0 GiB for an array with shape (99683, 99683) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m vectors \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(df_unique[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m----> 4\u001b[0m similarity \u001b[38;5;241m=\u001b[39m cosine_similarity(vectors)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1585\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1582\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1583\u001b[0m     Y_normalized \u001b[38;5;241m=\u001b[39m normalize(Y, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 1585\u001b[0m K \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X_normalized, Y_normalized\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39mdense_output)\n\u001b[0;32m   1587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m K\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\extmath.py:193\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    191\u001b[0m         ret \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(a, b)\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 193\u001b[0m     ret \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m@\u001b[39m b\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    196\u001b[0m     sparse\u001b[38;5;241m.\u001b[39missparse(a)\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    200\u001b[0m ):\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 74.0 GiB for an array with shape (99683, 99683) and data type float64"
     ]
    }
   ],
   "source": [
    "# Sử dụng TfidfVectorizer và tính cosine similarity trên cột 'feature'\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words=\"english\")\n",
    "vectors = tfidf_vectorizer.fit_transform(df_unique[\"title\"]).toarray()\n",
    "similarity = cosine_similarity(vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 72.915979,
   "end_time": "2022-12-01T10:46:08.278826",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-12-01T10:44:55.362847",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
